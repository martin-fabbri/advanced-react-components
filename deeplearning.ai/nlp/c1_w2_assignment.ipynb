{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c1_w2_assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjJXQ2InX0scoNgnIeBlsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/advanced-react-components/blob/master/deeplearning.ai/nlp/c1_w2_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2UVJ4inaHhA"
      },
      "source": [
        "# Assignment 2: Naive Bayes\n",
        "Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
        "\n",
        "* Train a naive bayes model on a sentiment analysis task\n",
        "* Test using your model\n",
        "* Compute ratios of positive words to negative words\n",
        "* Do some error analysis\n",
        "* Predict on your own tweet\n",
        "\n",
        "You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n",
        "* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiments.\n",
        "* This approach gives us simpler formulas for these 2-way classification tasks.\n",
        "\n",
        "Load the cell below to import some packages.\n",
        "You  may want to browse the documentation of unfamiliar libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajia0ZEVaGy2"
      },
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MGs_5bfaLPA",
        "outputId": "e30cf41e-5d97-4d01-f7d0-acd0ae535296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE1v2BtIaxfB",
        "outputId": "6ca0ed35-2a39-4b9f-bb68-1a5e1b6d720a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_x[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgA2NURnbewY",
        "outputId": "a978c3b4-5ba8-458f-ec64-4408f4004d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_x[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan',\n",
              " \"@heyclaireee is back! thnx God!!! i'm so happy :)\",\n",
              " '@BBCRadio3 thought it was my ears which were malfunctioning, thank goodness you cleared that one up with an apology :-)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMSDuTXb8lu"
      },
      "source": [
        "## Part 1: Process the Data\n",
        "\n",
        "For anu machine learning project, once you've gathered the data, the firts step is to make useful inputs to your model.\n",
        "\n",
        "- **Remove noise (stop words?)**: You will first want to remove noise from your data --that is, *remove words* that don't tell you much about the content. These include all common words like \"I, you, are, is, etc...\" that would not give us enough information on the sentiment.\n",
        "\n",
        "- **Remove symbols** such as stock market tickers, retweet symbols, hyperlink, and hastags because they cannot tell you a lot of information of the sentiment.\n",
        "\n",
        "- **Remove punctuation** from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\", \"happy.\" as different words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0YVDtLqbmdX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}