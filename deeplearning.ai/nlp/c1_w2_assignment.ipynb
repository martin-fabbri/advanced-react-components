{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c1_w2_assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyp2J94WkA0zKwTzhFTLB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/advanced-react-components/blob/master/deeplearning.ai/nlp/c1_w2_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2UVJ4inaHhA"
      },
      "source": [
        "# Assignment 2: Naive Bayes\n",
        "Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
        "\n",
        "* Train a naive bayes model on a sentiment analysis task\n",
        "* Test using your model\n",
        "* Compute ratios of positive words to negative words\n",
        "* Do some error analysis\n",
        "* Predict on your own tweet\n",
        "\n",
        "You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n",
        "* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiments.\n",
        "* This approach gives us simpler formulas for these 2-way classification tasks.\n",
        "\n",
        "Load the cell below to import some packages.\n",
        "You  may want to browse the documentation of unfamiliar libraries and functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajia0ZEVaGy2"
      },
      "source": [
        "import pdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTMiN3pB8DyZ",
        "outputId": "29af2915-f111-4ce0-8508-a2726c28e4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MGs_5bfaLPA"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE1v2BtIaxfB",
        "outputId": "d70e19de-8809-42d0-dc3a-b990995bb319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_x[:3]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgA2NURnbewY",
        "outputId": "4e3b9b98-6aaf-4e9e-a0e8-732c05d67e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_x[:3]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan',\n",
              " \"@heyclaireee is back! thnx God!!! i'm so happy :)\",\n",
              " '@BBCRadio3 thought it was my ears which were malfunctioning, thank goodness you cleared that one up with an apology :-)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMSDuTXb8lu"
      },
      "source": [
        "## Part 1. Process the Data\n",
        "\n",
        "For anu machine learning project, once you've gathered the data, the firts step is to make useful inputs to your model.\n",
        "\n",
        "- **Remove noise (stop words?)**: You will first want to remove noise from your data --that is, *remove words* that don't tell you much about the content. These include all common words like \"I, you, are, is, etc...\" that would not give us enough information on the sentiment.\n",
        "\n",
        "- **Remove symbols** such as stock market tickers, retweet symbols, hyperlink, and hastags because they cannot tell you a lot of information of the sentiment.\n",
        "\n",
        "- **Remove punctuation** from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\", \"happy.\" as different words.\n",
        "\n",
        "- **Stemm the words**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFDrjQ7P7wAo"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0YVDtLqbmdX",
        "outputId": "09f8b0ca-5ede-4ced-adfe-20bcd2a9da43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "process_tweet(custom_tweet)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'great', 'day', ':)', 'good', 'morn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Ffjjce8u2y"
      },
      "source": [
        "### Part 1.1. Implementing your helper functions\n",
        "\n",
        "To help train your naive bayes model, you will need to build a distionary hwre the keys are a (word, label) tuple and the values are the corresponding frequency. Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
        "\n",
        "You wil also implement a `lookup()` helper function that takes in the `freqs` dictionary, a word and a label to return the number of times that tuple appears in the collection of tweets.\n",
        "\n",
        "For example: given a list of tweets `[\"I am rather excited\", \"you are rather happy]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
        "\n",
        "{(\"raher\", 1): 2, (\"happi\", 1): 1, (\"excit\", 1): 1}\n",
        "\n",
        "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
        "\n",
        "- Notice how the words \"i\" and \"am\" are not saved, since they were removed as part of the cleaning process(stop words removal). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdRK2RA-7qE8"
      },
      "source": [
        "def count_tweets(results, tweets, ys):\n",
        "  '''\n",
        "  Input:\n",
        "    result: a dictionary that will be used to map each pair to its frequency \n",
        "    tweets: a list of tweets\n",
        "    ys: a list correspoding mapping each pair to its frequency\n",
        "\n",
        "  Output:\n",
        "    result: a dictionary mapping each pair to its frequency \n",
        "  '''\n",
        "  for y, tweet in zip(ys, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      result[pair] = result.get(pair, 0) + 1\n",
        "  return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noYkh_3b80nJ",
        "outputId": "847e6992-5118-47f4-e647-786027fffae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('sad', 0): 1, ('tire', 0): 2, ('trick', 0): 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPmnMkrAHaZG"
      },
      "source": [
        "Expected Output: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DVkOtT2HgmD"
      },
      "source": [
        "## Part 2. Train a Naive Bayes model\n",
        "\n",
        "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also a short prediction time.\n",
        "\n",
        "How do you train a Naive Bayes classifier?\n",
        "\n",
        "- The first step if to identify the number of classes.\n",
        "- Create a probability for each class. $P(D_{pos})$ is the probability that the document is positive. $P(D_{neg})$ is the probability that the document is negative. Use the formulas as follows and store the values in a dictionary:\n",
        "\n",
        "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
        "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "*   $D$ is the total number of documents, or tweets in this case.\n",
        "*   $D_{pos}$ is the total number of positive tweets\n",
        "*   $D_{neg}$ is the total number of negative tweets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcRAgSHKfxC"
      },
      "source": [
        "#### Prior and Logprior\n",
        "\n",
        "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
        "\n",
        "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
        "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
        "\n",
        "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
        "\n",
        "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
        "\n",
        "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-evZZeluNfDh"
      },
      "source": [
        "#### Positive and Negative Probability of a Word (Laplace Smoothing)\n",
        "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
        "\n",
        "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
        "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
        "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
        "\n",
        "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CepMyhIrN1X4"
      },
      "source": [
        "#### Log likelihood\n",
        "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
        "\n",
        "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU_5wOZKN2Ni"
      },
      "source": [
        "##### Create `freqs` dictionary\n",
        "- Given your `count_tweets()` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
        "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
        "- The value is the number of times it has appeared.\n",
        "\n",
        "We will use this dictionary in several parts of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I75gcMJvN0t3",
        "outputId": "44d0f38c-0a62-4873-b302-bf6f9452f0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Build the freqs dictionary for later uses\n",
        "\n",
        "freqs = count_tweets({}, train_x, train_y)\n",
        "len(freqs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mScdhN5DPeox"
      },
      "source": [
        "#### Instructions\n",
        "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
        "\n",
        "##### Calculate $V$\n",
        "- `V  is the number of unique words in the entire set of documents, for all classes, whether positive or negative.`\n",
        "\n",
        "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4lK4c0LQ2iu",
        "outputId": "9a27419b-0ff1-43ec-f2ad-8bbf47b8dafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "V = set(word for word, _ in freqs)\n",
        "len(V)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3byHDIiQ3YZ"
      },
      "source": [
        "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
        "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
        "\n",
        "##### Calculate $N_{pos}$ and $N_{neg}$\n",
        "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
        "\n",
        "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
        "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
        "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
        "\n",
        "##### Calculate the logprior\n",
        "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
        "\n",
        "##### Calculate log likelihood\n",
        "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
        "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
        "\n",
        "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgNQKES-80su",
        "outputId": "c70049f5-8380-497b-e03b-482dee9b6beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    V = set(word for word, _ in freqs)\n",
        "\n",
        "    # calculate N_pos and N_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += None\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += None\n",
        "\n",
        "    # Calculate D, the number of documents\n",
        "    D = None\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
        "    D_pos = None\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
        "    D_neg = None\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior = None\n",
        "\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = None\n",
        "        freq_neg = None\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = None\n",
        "        p_w_neg = None\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ab', 'bs', 'rr'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lua1dAcm80zo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCae_FFj805b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qciaRQcK80-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24281Ng281Bg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NaALc70808V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e41FPiOc802h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vNGQ-Sh80wa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11yUq5KT80qI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-sJWnmo80kj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}